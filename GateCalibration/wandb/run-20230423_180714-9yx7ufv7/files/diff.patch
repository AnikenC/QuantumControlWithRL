diff --git a/GateCalibration/environment.py b/GateCalibration/environment.py
index 0d5ad0e..688505e 100644
--- a/GateCalibration/environment.py
+++ b/GateCalibration/environment.py
@@ -9,7 +9,7 @@ from static import AbstractionLevel
 class GateCalibrationEnvironment(gym.Env):
     metadata = {"render_modes": ["human"]}
 
-    def __init__(self, render_mode=None):
+    def __init__(self):
         self.qenvironment = QuantumEnvironment(target=CNOT(), abstraction_level=AbstractionLevel.CIRCUIT)
         self.init_obs = np.ones(1, dtype=np.float64)
         self.n_actions = 7
@@ -17,6 +17,10 @@ class GateCalibrationEnvironment(gym.Env):
         self.max_reward = 0.
         self.step_for_max_reward = 0
         self.episode_length = 0
+        self.simple_sample = True
+        self.simple_size = 2
+        assert type(self.simple_size) == int, "Type for sampling must be int"
+        assert self.simple_size > 0 and self.simple_size < 16, "Sample Size must be greater than 0 and less than 16"
 
         self.action_space = Box(
             low=-1.0, high=1.0, shape=(self.n_actions,), dtype=np.float64
@@ -50,7 +54,9 @@ class GateCalibrationEnvironment(gym.Env):
         ### Single Length Episode ###
         self.episode_length += 1
 
-        index = 0 # np.random.randint(16)
+        index = np.random.randint(16)
+        if self.simple_sample:
+            index = np.random.randint(self.simple_size)
         self.reward = self.qenvironment.perform_action_gate_cal(action, index) # Can support batched actions
         if np.max(self.reward) > self.max_reward:
             self.max_reward = np.max(self.reward)
diff --git a/GateCalibration/ppo_train.py b/GateCalibration/ppo_train.py
index 16da5da..6c0ec34 100644
--- a/GateCalibration/ppo_train.py
+++ b/GateCalibration/ppo_train.py
@@ -29,9 +29,9 @@ def parse_args():
         help="if toggled, cuda will be enabled by default")
     parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
         help="if toggled, this experiment will be tracked with Weights and Biases")
-    parser.add_argument("--wandb-project-name", type=str, default="cleanRL",
+    parser.add_argument("--wandb-project-name", type=str, default="GateCalibration",
         help="the wandb's project name")
-    parser.add_argument("--wandb-entity", type=str, default=None,
+    parser.add_argument("--wandb-entity", type=str, default="quantumcontrolwithrl",
         help="the entity (team) of wandb's project")
     parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
         help="whether to capture videos of the agent performances (check out `videos` folder)")
@@ -223,13 +223,12 @@ if __name__ == "__main__":
                 if info is None:
                     continue
                 temp_return = info["mean reward"]
-                delta = info["mean reward"] - np.mean(reward)
                 max_reward_at_step = info["step for max"]
                 max_reward = info["max reward"]
                 #print(f"global_step={global_step}, episodic_return={temp_return}, delta={delta}")
                 #print(f"max reward of {max_reward} at step {max_reward_at_step}")
-                print(f"delta: {delta}")
                 writer.add_scalar("charts/episodic_return", temp_return, global_step)
+                writer.add_scalar("charts/normalized_episodic_return", np.mean(reward), global_step)
                 writer.add_scalar("charts/episodic_length", info["episode length"], global_step)
 
         # bootstrap value if not done
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 215b7b2..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-qiskit
-qiskit_ibm_runtime
-tensorflow
\ No newline at end of file
